{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the prompts\n",
    "prompts = [\n",
    "    \"A farmer has chickens and cows on his farm. There are a total of 30 heads and 100 legs. How many chickens and how many cows are there?\",\n",
    "    \"Observe the following sequence of numbers: 2, 4, 8, 16. Predict the next number in the sequence and explain the pattern.\",\n",
    "    \"Hand is to glove as foot is to ____?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TinyLlama model and tokenizer\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tinyllama_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Load the DeepSeek R1 Distill Qwen 1.5B model and tokenizer\n",
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Function to generate a response from a model\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    # Enhance the prompt with summarization instructions\n",
    "    instruction = \"Please provide a complete and concise summary of the following within 512 tokens:\\n\"\n",
    "    enhanced_prompt = instruction + prompt\n",
    "\n",
    "    # Tokenize the enhanced prompt\n",
    "    inputs = tokenizer(enhanced_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,  # Limit the generated response to 512 tokens\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    # Extract the generated tokens, excluding the input prompt\n",
    "    generated_tokens = outputs[:, inputs['input_ids'].shape[-1]:]\n",
    "    response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the prompts and get responses from both models\n",
    "for prompt in prompts:\n",
    "    tinyllama_response = generate_response(tinyllama_model, tinyllama_tokenizer, prompt)\n",
    "    deepseek_response = generate_response(deepseek_model, deepseek_tokenizer, prompt)\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"TinyLlama\": tinyllama_response,\n",
    "        \"DeepSeek R1 Distill Qwen 1.5B\": deepseek_response\n",
    "    })\n",
    "\n",
    "# Initialize the console for rich output\n",
    "console = Console()\n",
    "\n",
    "# Display each prompt and its corresponding responses\n",
    "for result in results:\n",
    "    markdown_content = f\"\"\"\n",
    "**Prompt:**\n",
    "\n",
    "{result['Prompt']}\n",
    "\n",
    "**TinyLlama:**\n",
    "\n",
    "{result['TinyLlama']}\n",
    "\n",
    "**DeepSeek R1 Distill Qwen 1.5B:**\n",
    "\n",
    "{result['DeepSeek R1 Distill Qwen 1.5B']}\n",
    "\"\"\"\n",
    "    console.print(Markdown(markdown_content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
